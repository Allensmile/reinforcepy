!_TAG_FILE_FORMAT	2	/extended format; --format=1 will not append ;" to lines/
!_TAG_FILE_SORTED	1	/0=unsorted, 1=sorted, 2=foldcase/
!_TAG_PROGRAM_AUTHOR	Darren Hiebert	/dhiebert@users.sourceforge.net/
!_TAG_PROGRAM_NAME	Exuberant Ctags	//
!_TAG_PROGRAM_URL	http://ctags.sourceforge.net	/official site/
!_TAG_PROGRAM_VERSION	5.9~svn20110310	//
ACTION_NAMES	examples/SimpleQLearn/FrozenLakev0.py	/^ACTION_NAMES = ['l', 'd', 'u', 'r']$/;"	kind:variable	line:7
ALEEnvironment	reinforcepy/environments/ALE/ALE_environment.py	/^class ALEEnvironment(BaseEnvironment):$/;"	kind:class	line:7
ALE_environment.py	reinforcepy/environments/ALE/ALE_environment.py	1;"	kind:file	line:1
ActionHandler	reinforcepy/handlers/actionhandler.py	/^class ActionHandler:$/;"	kind:class	line:20
ActionPolicy	reinforcepy/handlers/actionhandler.py	/^class ActionPolicy(Enum):$/;"	kind:class	line:6
AsyncA3CCNN	reinforcepy/networks/dqn/theanolasagne/async_a3c_cnn.py	/^class AsyncA3CCNN:$/;"	kind:class	line:7
AsyncNStepNoveltyLearner	reinforcepy/learners/Novelty/NoveltyClient.py	/^class AsyncNStepNoveltyLearner(BaseAsyncProcessTargetLearner, AsyncNStepDQNLearner):$/;"	kind:class	line:5
AsyncTargetCNN	reinforcepy/networks/dqn/theanolasagne/async_target_cnn.py	/^class AsyncTargetCNN:$/;"	kind:class	line:8
AsyncTargetCNNNstep	reinforcepy/networks/dqn/theanolasagne/async_target_cnn.py	/^class AsyncTargetCNNNstep(AsyncTargetCNN):$/;"	kind:class	line:180
AsyncTargetCNNSarsa	reinforcepy/networks/dqn/theanolasagne/async_target_cnn.py	/^class AsyncTargetCNNSarsa(AsyncTargetCNN):$/;"	kind:class	line:143
AsyncThreadHost	reinforcepy/learners/dqn/asynchronous/async_thread_host.py	/^class AsyncThreadHost:$/;"	kind:class	line:6
AtariGym	reinforcepy/environments/openaigym/gym_atari.py	/^class AtariGym(BaseEnvironment):$/;"	kind:class	line:5
BaseEnvironment	reinforcepy/environments/base_environment.py	/^class BaseEnvironment(metaclass=ABCMeta):$/;"	kind:class	line:4
BaseLearner	reinforcepy/learners/base_learner.py	/^class BaseLearner(metaclass=ABCMeta):$/;"	kind:class	line:4
BaseNetwork	reinforcepy/networks/dqn/base_network.py	/^class BaseNetwork:$/;"	kind:class	line:4
BaseQLearner	reinforcepy/learners/base_q_learner.py	/^class BaseQLearner(BaseLearner):$/;"	kind:class	line:4
BaseSarsaLearner	reinforcepy/learners/base_sarsa_learner.py	/^class BaseSarsaLearner(BaseLearner):$/;"	kind:class	line:4
BaseThreadLearner	reinforcepy/learners/dqn/asynchronous/base_thread_learner.py	/^class BaseThreadLearner(threading.Thread):$/;"	kind:class	line:7
BinaryTree	reinforcepy/handlers/experience_replay/prioritized/binarytree.py	/^class BinaryTree:$/;"	kind:class	line:92
CONFIG	examples/ALE/DQN_Async/run_a3c.py	/^    CONFIG = json.load(open('a3c_cfg.json'))$/;"	kind:variable	line:34
CONFIG	examples/ALE/DQN_Async/run_a3c_lstm.py	/^    CONFIG = json.load(open('a3c_cfg.json'))$/;"	kind:variable	line:34
CONFIG	examples/ALE/DQN_Async/run_a3c_viewer.py	/^CONFIG = json.load(open('a3c_cfg.json'))$/;"	kind:variable	line:9
CONFIG	examples/ALE/DQN_Async/run_dqn.py	/^    CONFIG = json.load(open('dqn_cfg.json'))$/;"	kind:variable	line:41
CONFIG	examples/ALE/DQN_Async/run_dqn_viewer.py	/^CONFIG = json.load(open('dqn_cfg.json'))$/;"	kind:variable	line:8
CONFIG	examples/ALE/DQN_Async/run_dqnexpreplay.py	/^    CONFIG = json.load(open('dqn_cfg.json'))$/;"	kind:variable	line:41
CONFIG	examples/ALE/DQN_Async/run_onestep_sarsa.py	/^CONFIG = json.load(open('onestep_cfg.json'))$/;"	kind:variable	line:7
DQNLearner	reinforcepy/learners/dqn/dqn.py	/^class DQNLearner(BaseQLearner):$/;"	kind:class	line:7
DQN_NIPS	reinforcepy/networks/dqn/tflow/dqn_nips.py	/^class DQN_NIPS:$/;"	kind:class	line:5
DQN_NIPS	reinforcepy/networks/dqn/theanolasagne/dqn_nips.py	/^class DQN_NIPS:$/;"	kind:class	line:10
DataSet	reinforcepy/handlers/experience_replay/dataset.py	/^class DataSet():$/;"	kind:class	line:12
EPOCH_DEF	examples/ALE/DQN/run_dqn_experiment.py	/^EPOCH_DEF = 50000  # an epoch is defined as 50,000 steps in the NIPS paper$/;"	kind:variable	line:5
ExpQThreadLearner	reinforcepy/learners/dqn/asynchronous/exp_replay_q_thread_learner.py	/^class ExpQThreadLearner(BaseThreadLearner, BaseQLearner):$/;"	kind:class	line:7
FLOATX	reinforcepy/networks/dqn/theanolasagne/dqn_nips.py	/^FLOATX = theano.config.floatX$/;"	kind:variable	line:7
FrameBuffer	reinforcepy/handlers/framebuffer.py	/^class FrameBuffer:$/;"	kind:class	line:4
FrozenLakev0.py	examples/SimpleQLearn/FrozenLakev0.py	1;"	kind:file	line:1
GymWrapper	reinforcepy/environments/openaigym/gym_wrapper.py	/^class GymWrapper(BaseEnvironment):$/;"	kind:class	line:4
LinnearAnnealer	reinforcepy/handlers/linear_annealer.py	/^class LinnearAnnealer:$/;"	kind:class	line:4
MAX_EPISODES	examples/SimpleQLearn/FrozenLakev0.py	/^MAX_EPISODES = 5000$/;"	kind:variable	line:14
NStepA3C	reinforcepy/networks/dqn/tflow/nstep_a3c.py	/^class NStepA3C(TargetDQN):$/;"	kind:class	line:10
NStepA3CLSTM	reinforcepy/networks/dqn/tflow/nstep_a3c_lstm.py	/^class NStepA3CLSTM(TargetDQN):$/;"	kind:class	line:39
Node	reinforcepy/handlers/experience_replay/prioritized/binarytree.py	/^class Node:$/;"	kind:class	line:5
NoveltyClient.py	reinforcepy/learners/Novelty/NoveltyClient.py	1;"	kind:file	line:1
NoveltyHost	reinforcepy/learners/Novelty/NoveltyHost.py	/^class NoveltyHost(AsyncLearnerHost):$/;"	kind:class	line:5
NoveltyHost.py	reinforcepy/learners/Novelty/NoveltyHost.py	1;"	kind:file	line:1
OneStepSARSAThreadLearner	reinforcepy/learners/dqn/asynchronous/onestep_sarsa_thread_learner.py	/^class OneStepSARSAThreadLearner(OneStepBaseThreadLearner, BaseSarsaLearner):$/;"	kind:class	line:6
OneStepTargetSARSA	reinforcepy/networks/dqn/tflow/one_step_target_sarsa.py	/^class OneStepTargetSARSA:$/;"	kind:class	line:8
Parameters	reinforcepy/handlers/parameters.py	/^class Parameters:$/;"	kind:class	line:1
PyTest	setup.py	/^class PyTest(TestCommand):$/;"	kind:class	line:25
QTable	reinforcepy/learners/TableLookup/qtable.py	/^class QTable(BaseQLearner):$/;"	kind:class	line:6
QThreadLearner	reinforcepy/learners/dqn/asynchronous/q_thread_learner.py	/^class QThreadLearner(BaseThreadLearner, BaseQLearner):$/;"	kind:class	line:6
RecurrentThreadLearner	reinforcepy/learners/dqn/asynchronous/recurrent_thread_learner.py	/^class RecurrentThreadLearner(QThreadLearner):$/;"	kind:class	line:5
SARSATable	reinforcepy/learners/TableLookup/sarsatable.py	/^class SARSATable(QTable, BaseSarsaLearner):$/;"	kind:class	line:7
TargetDQN	reinforcepy/networks/dqn/tflow/target_dqn.py	/^class TargetDQN(BaseNetwork):$/;"	kind:class	line:11
TorchInit	reinforcepy/networks/dqn/theanolasagne/dqn_inits.py	/^class TorchInit(lasagne.init.Initializer):$/;"	kind:class	line:110
__init__	reinforcepy/environments/ALE/ALE_environment.py	/^    def __init__(self, rom, resize_shape=(84, 84), skip_frame=1, repeat_action_probability=0.0,$/;"	kind:member	line:20
__init__	reinforcepy/environments/openaigym/gym_atari.py	/^    def __init__(self, env):$/;"	kind:member	line:6
__init__	reinforcepy/environments/openaigym/gym_wrapper.py	/^    def __init__(self, env):$/;"	kind:member	line:5
__init__	reinforcepy/handlers/actionhandler.py	/^    def __init__(self, num_actions: int, random_values, action_policy: ActionPolicy=ActionPolicy.eGreedy):$/;"	kind:member	line:40
__init__	reinforcepy/handlers/experience_replay/dataset.py	/^    def __init__(self, width, height, rng=np.random.RandomState(), max_steps=1000, phi_length=4):$/;"	kind:member	line:17
__init__	reinforcepy/handlers/experience_replay/prioritized/binarytree.py	/^    def __init__(self):$/;"	kind:member	line:93
__init__	reinforcepy/handlers/experience_replay/prioritized/binarytree.py	/^    def __init__(self, value, extra_vals=None):$/;"	kind:member	line:6
__init__	reinforcepy/handlers/framebuffer.py	/^    def __init__(self, shape, dtype=np.uint8):$/;"	kind:member	line:18
__init__	reinforcepy/handlers/linear_annealer.py	/^    def __init__(self, start, end, steps):$/;"	kind:member	line:5
__init__	reinforcepy/handlers/parameters.py	/^    def __init__(self, name, values):$/;"	kind:member	line:14
__init__	reinforcepy/learners/Novelty/NoveltyClient.py	/^    def __init__(self, learner_parms, network_partial, pipe):$/;"	kind:member	line:6
__init__	reinforcepy/learners/Novelty/NoveltyHost.py	/^    def __init__(self, host_cnn, novel_states, learners, environment_partials):$/;"	kind:member	line:6
__init__	reinforcepy/learners/TableLookup/qtable.py	/^    def __init__(self, action_size, anneal_tuple, learning_rate=0.1, discount=0.99):$/;"	kind:member	line:7
__init__	reinforcepy/learners/dqn/asynchronous/async_thread_host.py	/^    def __init__(self, network, log_dir):$/;"	kind:member	line:7
__init__	reinforcepy/learners/dqn/asynchronous/base_thread_learner.py	/^    def __init__(self, environment, network, global_dict, phi_length=4,$/;"	kind:member	line:8
__init__	reinforcepy/learners/dqn/asynchronous/exp_replay_q_thread_learner.py	/^    def __init__(self, environment, network, global_dict, dataset_size, batch_size, **kwargs):$/;"	kind:member	line:8
__init__	reinforcepy/learners/dqn/asynchronous/recurrent_thread_learner.py	/^    def __init__(self, *args, **kwargs):$/;"	kind:member	line:6
__init__	reinforcepy/learners/dqn/dqn.py	/^    def __init__(self, learner_parms, network, batch_size=32, testing=False):$/;"	kind:member	line:8
__init__	reinforcepy/networks/dqn/base_network.py	/^    def __init__(self, input_shape, output_num):$/;"	kind:member	line:10
__init__	reinforcepy/networks/dqn/tflow/dqn_nips.py	/^    def __init__(self, network_parms, training_parms, log_dir=None, log_steps=100, log_metadata=False):$/;"	kind:member	line:6
__init__	reinforcepy/networks/dqn/tflow/nstep_a3c.py	/^    def __init__(self, input_shape, output_num, optimizer=None, network_generator=tf_util.create_a3c_network, q_discount=0.99,$/;"	kind:member	line:11
__init__	reinforcepy/networks/dqn/tflow/nstep_a3c_lstm.py	/^    def __init__(self, input_shape, output_num, optimizer=None, network_generator=create_a3c_lstm_network, q_discount=0.99,$/;"	kind:member	line:40
__init__	reinforcepy/networks/dqn/tflow/one_step_target_sarsa.py	/^    def __init__(self, input_shape, output_num, optimizer=None, network_generator=None, q_discount=0.99, loss_clipping=1):$/;"	kind:member	line:9
__init__	reinforcepy/networks/dqn/tflow/target_dqn.py	/^    def __init__(self, input_shape, output_num, algorithm_type, optimizer=None, network_generator=tf_util.create_nips_network, q_discount=0.99, loss_clipping=1,$/;"	kind:member	line:16
__init__	reinforcepy/networks/dqn/theanolasagne/async_a3c_cnn.py	/^    def __init__(self, inp_shape, output_num, training_size, stride=(4, 2), untie_biases=False):$/;"	kind:member	line:8
__init__	reinforcepy/networks/dqn/theanolasagne/async_target_cnn.py	/^    def __init__(self, network_parms, training_parms):$/;"	kind:member	line:181
__init__	reinforcepy/networks/dqn/theanolasagne/async_target_cnn.py	/^    def __init__(self, network_parms, training_parms):$/;"	kind:member	line:9
__init__	reinforcepy/networks/dqn/theanolasagne/dqn_inits.py	/^    def __init__(self, fan_in):$/;"	kind:member	line:111
__init__	reinforcepy/networks/dqn/theanolasagne/dqn_nips.py	/^    def __init__(self, network_parms, training_parms):$/;"	kind:member	line:11
__init__.py	reinforcepy/__init__.py	1;"	kind:file	line:1
__init__.py	reinforcepy/environments/ALE/__init__.py	1;"	kind:file	line:1
__init__.py	reinforcepy/environments/__init__.py	1;"	kind:file	line:1
__init__.py	reinforcepy/environments/openaigym/__init__.py	1;"	kind:file	line:1
__init__.py	reinforcepy/handlers/__init__.py	1;"	kind:file	line:1
__init__.py	reinforcepy/handlers/experience_replay/__init__.py	1;"	kind:file	line:1
__init__.py	reinforcepy/handlers/experience_replay/prioritized/__init__.py	1;"	kind:file	line:1
__init__.py	reinforcepy/learners/Novelty/__init__.py	1;"	kind:file	line:1
__init__.py	reinforcepy/learners/TableLookup/__init__.py	1;"	kind:file	line:1
__init__.py	reinforcepy/learners/__init__.py	1;"	kind:file	line:1
__init__.py	reinforcepy/learners/dqn/__init__.py	1;"	kind:file	line:1
__init__.py	reinforcepy/learners/dqn/asynchronous/__init__.py	1;"	kind:file	line:1
__init__.py	reinforcepy/networks/__init__.py	1;"	kind:file	line:1
__init__.py	reinforcepy/networks/dqn/__init__.py	1;"	kind:file	line:1
__init__.py	reinforcepy/networks/dqn/tflow/__init__.py	1;"	kind:file	line:1
__init__.py	reinforcepy/networks/dqn/theanolasagne/__init__.py	1;"	kind:file	line:1
__init__.py	reinforcepy/networks/util/__init__.py	1;"	kind:file	line:1
__len__	reinforcepy/handlers/experience_replay/dataset.py	/^    def __len__(self):$/;"	kind:member	line:69
_create_tf_saver	reinforcepy/networks/dqn/base_network.py	/^    def _create_tf_saver(self):$/;"	kind:member	line:37
_get_output	reinforcepy/networks/dqn/tflow/target_dqn.py	/^    def _get_output(self, sess, state):$/;"	kind:member	line:204
_step	reinforcepy/environments/ALE/ALE_environment.py	/^    def _step(self, ale_action):$/;"	kind:member	line:63
_train_step	reinforcepy/networks/dqn/tflow/target_dqn.py	/^    def _train_step(self, sess, states, actions, rewards, states_tp1, terminals, global_step=0, summaries=False):$/;"	kind:member	line:161
a	examples/ALE/Human_Supervision/generate_human_dataset.py	/^    a = key_action_tform_table[keys]$/;"	kind:variable	line:103
accumulate_gradients	reinforcepy/networks/dqn/theanolasagne/async_a3c_cnn.py	/^    def accumulate_gradients(self, states, actions, rewards):$/;"	kind:member	line:93
accumulate_gradients	reinforcepy/networks/dqn/theanolasagne/async_target_cnn.py	/^    def accumulate_gradients(self, state, action, reward, action_tp1, state_tp1, terminal):$/;"	kind:member	line:167
accumulate_gradients	reinforcepy/networks/dqn/theanolasagne/async_target_cnn.py	/^    def accumulate_gradients(self, state, action, reward, state_tp1, terminal):$/;"	kind:member	line:89
accumulate_gradients	reinforcepy/networks/dqn/theanolasagne/async_target_cnn.py	/^    def accumulate_gradients(self, states, actions, rewards):$/;"	kind:member	line:205
action_handler	reinforcepy/test/test_actionhandler.py	/^def action_handler():$/;"	kind:function	line:7
actionhandler.py	reinforcepy/handlers/actionhandler.py	1;"	kind:file	line:1
actions	examples/ALE/Human_Supervision/generate_human_dataset.py	/^actions = list()$/;"	kind:variable	line:85
add_sample	reinforcepy/handlers/experience_replay/dataset.py	/^    def add_sample(self, img, action, reward, terminal):$/;"	kind:member	line:48
add_state_to_buffer	reinforcepy/handlers/framebuffer.py	/^    def add_state_to_buffer(self, state):$/;"	kind:member	line:24
ale	examples/ALE/Human_Supervision/generate_human_dataset.py	/^ale = ALEInterface(False)$/;"	kind:variable	line:57
anneal	reinforcepy/handlers/actionhandler.py	/^    def anneal(self):$/;"	kind:member	line:97
anneal	reinforcepy/handlers/linear_annealer.py	/^    def anneal(self):$/;"	kind:member	line:16
anneal_learning_rate	reinforcepy/networks/dqn/tflow/target_dqn.py	/^    def anneal_learning_rate(self, global_step):$/;"	kind:member	line:215
anneal_random_policy	reinforcepy/learners/dqn/asynchronous/base_thread_learner.py	/^    def anneal_random_policy(self):$/;"	kind:member	line:63
anneal_to	reinforcepy/handlers/actionhandler.py	/^    def anneal_to(self, anneal_count):$/;"	kind:member	line:105
anneal_to	reinforcepy/handlers/linear_annealer.py	/^    def anneal_to(self, anneal_count):$/;"	kind:member	line:24
anneal_tuple	examples/SimpleQLearn/FrozenLakev0.py	/^anneal_tuple = (1, 0.01, 1000)  # start eGreedy from 1 - 0.01 for 1000 episodes$/;"	kind:variable	line:10
async_a3c_cnn.py	reinforcepy/networks/dqn/theanolasagne/async_a3c_cnn.py	1;"	kind:file	line:1
async_target_cnn.py	reinforcepy/networks/dqn/theanolasagne/async_target_cnn.py	1;"	kind:file	line:1
async_thread_host.py	reinforcepy/learners/dqn/asynchronous/async_thread_host.py	1;"	kind:file	line:1
author	docs/conf.py	/^author = 'IslandMan93'$/;"	kind:variable	line:67
author	setup.py	/^    author='IslandMan93',$/;"	kind:variable	line:41
author_email	setup.py	/^    author_email='islandman93@gmail.com',$/;"	kind:variable	line:45
base_environment.py	reinforcepy/environments/base_environment.py	1;"	kind:file	line:1
base_learner.py	reinforcepy/learners/base_learner.py	1;"	kind:file	line:1
base_network.py	reinforcepy/networks/dqn/base_network.py	1;"	kind:file	line:1
base_q_learner.py	reinforcepy/learners/base_q_learner.py	1;"	kind:file	line:1
base_sarsa_learner.py	reinforcepy/learners/base_sarsa_learner.py	1;"	kind:file	line:1
base_thread_learner.py	reinforcepy/learners/dqn/asynchronous/base_thread_learner.py	1;"	kind:file	line:1
bias_variable_torch	reinforcepy/networks/util/tflow_util.py	/^def bias_variable_torch(name, shape, stdv):$/;"	kind:function	line:97
binarytree.py	reinforcepy/handlers/experience_replay/prioritized/binarytree.py	1;"	kind:file	line:1
btree	reinforcepy/test/test_binarytree.py	/^def btree():$/;"	kind:function	line:83
calc_novel_reward	reinforcepy/learners/Novelty/NoveltyClient.py	/^    def calc_novel_reward(self, old_frame, new_frame):$/;"	kind:member	line:19
classifiers	setup.py	/^    classifiers = [$/;"	kind:variable	line:52
clear_gradients	reinforcepy/networks/dqn/theanolasagne/async_a3c_cnn.py	/^    def clear_gradients(self):$/;"	kind:member	line:147
clear_gradients	reinforcepy/networks/dqn/theanolasagne/async_target_cnn.py	/^    def clear_gradients(self):$/;"	kind:member	line:139
clock	examples/ALE/Human_Supervision/generate_human_dataset.py	/^clock = pygame.time.Clock()$/;"	kind:variable	line:79
cmdclass	setup.py	/^    cmdclass={'test': PyTest},$/;"	kind:variable	line:44
compare	reinforcepy/test/test_binarytree.py	/^    def compare(ind):$/;"	kind:function	line:70
compute_variable_summaries	reinforcepy/networks/util/tflow_util.py	/^def compute_variable_summaries(var, name):$/;"	kind:function	line:59
conf.py	docs/conf.py	1;"	kind:file	line:1
convert_image	reinforcepy/environments/openaigym/gym_atari.py	/^    def convert_image(image):$/;"	kind:member	line:37
copy_parms_to_target	reinforcepy/networks/dqn/theanolasagne/async_target_cnn.py	/^    def copy_parms_to_target(self):$/;"	kind:member	line:133
copyright	docs/conf.py	/^copyright = '2016, IslandMan93'$/;"	kind:variable	line:66
create_A3C	reinforcepy/networks/dqn/theanolasagne/dqn_inits.py	/^def create_A3C(network_parms):$/;"	kind:function	line:22
create_NIPS	reinforcepy/networks/dqn/theanolasagne/dqn_inits.py	/^def create_NIPS(network_parms):$/;"	kind:function	line:4
create_NIPS_sprag_init	reinforcepy/networks/dqn/theanolasagne/dqn_inits.py	/^def create_NIPS_sprag_init(network_parms):$/;"	kind:function	line:40
create_a3c_lstm_network	reinforcepy/networks/dqn/tflow/nstep_a3c_lstm.py	/^def create_a3c_lstm_network(input_tensor, output_num):$/;"	kind:function	line:9
create_a3c_network	reinforcepy/networks/util/tflow_util.py	/^def create_a3c_network(input_tensor, output_num):$/;"	kind:function	line:130
create_async_muupan_init	reinforcepy/networks/dqn/theanolasagne/dqn_inits.py	/^def create_async_muupan_init(network_parms):$/;"	kind:function	line:66
create_loss_shared_vars	reinforcepy/networks/dqn/theanolasagne/async_target_cnn.py	/^    def create_loss_shared_vars(self):$/;"	kind:member	line:144
create_loss_shared_vars	reinforcepy/networks/dqn/theanolasagne/async_target_cnn.py	/^    def create_loss_shared_vars(self):$/;"	kind:member	line:194
create_loss_shared_vars	reinforcepy/networks/dqn/theanolasagne/async_target_cnn.py	/^    def create_loss_shared_vars(self):$/;"	kind:member	line:69
create_network_graph	reinforcepy/networks/dqn/base_network.py	/^    def create_network_graph(self):$/;"	kind:member	line:34
create_network_graph	reinforcepy/networks/dqn/tflow/nstep_a3c.py	/^    def create_network_graph(self):$/;"	kind:member	line:18
create_network_graph	reinforcepy/networks/dqn/tflow/nstep_a3c_lstm.py	/^    def create_network_graph(self):$/;"	kind:member	line:49
create_network_graph	reinforcepy/networks/dqn/tflow/one_step_target_sarsa.py	/^    def create_network_graph(self, input_shape, output_num, network_generator, q_discount, optimizer, loss_clipping):$/;"	kind:member	line:43
create_network_graph	reinforcepy/networks/dqn/tflow/target_dqn.py	/^    def create_network_graph(self):$/;"	kind:member	line:41
create_nips_network	reinforcepy/networks/dqn/tflow/dqn_inits.py	/^def create_nips_network(input_tensor, network_parms, input_shape, variable_summaries=True):$/;"	kind:function	line:88
create_nips_network	reinforcepy/networks/dqn/tflow/one_step_target_sarsa.py	/^def create_nips_network(input_tensor, output_num):$/;"	kind:function	line:192
create_nips_network	reinforcepy/networks/util/tflow_util.py	/^def create_nips_network(input_tensor, output_num):$/;"	kind:function	line:121
curractions	examples/ALE/Human_Supervision/generate_human_dataset.py	/^curractions = list()$/;"	kind:variable	line:88
current_learning_rate	reinforcepy/networks/dqn/tflow/target_dqn.py	/^    def current_learning_rate(self):$/;"	kind:member	line:219
currrewards	examples/ALE/Human_Supervision/generate_human_dataset.py	/^currrewards = list()$/;"	kind:variable	line:89
currstates	examples/ALE/Human_Supervision/generate_human_dataset.py	/^currstates = list()$/;"	kind:variable	line:90
dataset.py	reinforcepy/handlers/experience_replay/dataset.py	1;"	kind:file	line:1
deepmind_rmsprop	reinforcepy/networks/dqn/theanolasagne/dqn_rmsprop.py	/^def deepmind_rmsprop(loss_or_grads, params, learning_rate=1.0, rho=0.9, epsilon=1e-6):$/;"	kind:function	line:57
depth	reinforcepy/handlers/experience_replay/prioritized/binarytree.py	/^    def depth(self):$/;"	kind:member	line:80
description	setup.py	/^    description='Collection of reinforcement learners implemented in python.',$/;"	kind:variable	line:46
diff	examples/ALE/transfer_learning/transfertest.py	/^            diff = net - net_a._t_network_output$/;"	kind:variable	line:18
dqn.py	reinforcepy/learners/dqn/dqn.py	1;"	kind:file	line:1
dqn_inits.py	reinforcepy/networks/dqn/tflow/dqn_inits.py	1;"	kind:file	line:1
dqn_inits.py	reinforcepy/networks/dqn/theanolasagne/dqn_inits.py	1;"	kind:file	line:1
dqn_nips.py	reinforcepy/networks/dqn/tflow/dqn_nips.py	1;"	kind:file	line:1
dqn_nips.py	reinforcepy/networks/dqn/theanolasagne/dqn_nips.py	1;"	kind:file	line:1
dqn_nips_network	reinforcepy/networks/dqn/tflow/dqn_inits.py	/^def dqn_nips_network(network_parms, training_parms):$/;"	kind:function	line:5
dqn_rmsprop.py	reinforcepy/networks/dqn/theanolasagne/dqn_rmsprop.py	1;"	kind:file	line:1
eGreedy	reinforcepy/handlers/actionhandler.py	/^    eGreedy = 1$/;"	kind:variable	line:16
env	examples/SimpleQLearn/FrozenLakev0.py	/^env = GymWrapper(gym.make('FrozenLake-v0'))$/;"	kind:variable	line:8
environment	examples/ALE/transfer_learning/transfertest.py	/^environment = ALEEnvironment('\/home\/ben\/_code\/reinforcepy\/examples\/ALE\/roms\/breakout.bin')$/;"	kind:variable	line:7
environments	_experiments/test_api.py	/^environments = [env for env in list()]$/;"	kind:variable	line:2
episode	examples/ALE/Human_Supervision/generate_human_dataset.py	/^        episode = episode + 1$/;"	kind:variable	line:192
episode	examples/ALE/Human_Supervision/generate_human_dataset.py	/^episode = 0$/;"	kind:variable	line:82
episode_end	reinforcepy/learners/TableLookup/qtable.py	/^    def episode_end(self):$/;"	kind:member	line:51
episode_end	reinforcepy/learners/base_q_learner.py	/^    def episode_end(self):$/;"	kind:member	line:17
episode_end	reinforcepy/learners/base_sarsa_learner.py	/^    def episode_end(self):$/;"	kind:member	line:17
episode_frame_number	examples/ALE/Human_Supervision/generate_human_dataset.py	/^        episode_frame_number = ale.getEpisodeFrameNumber()$/;"	kind:variable	line:186
exclude_patterns	docs/conf.py	/^exclude_patterns = ['_build']$/;"	kind:variable	line:93
exit	examples/ALE/Human_Supervision/generate_human_dataset.py	/^            exit=True$/;"	kind:variable	line:175
exit	examples/ALE/Human_Supervision/generate_human_dataset.py	/^        exit = True$/;"	kind:variable	line:178
exit	examples/ALE/Human_Supervision/generate_human_dataset.py	/^    exit=False$/;"	kind:variable	line:172
exp_replay_q_thread_learner.py	reinforcepy/learners/dqn/asynchronous/exp_replay_q_thread_learner.py	1;"	kind:file	line:1
extensions	docs/conf.py	/^extensions = [$/;"	kind:variable	line:42
extras_require	setup.py	/^    extras_require={$/;"	kind:variable	line:64
finalize_options	setup.py	/^    def finalize_options(self):$/;"	kind:member	line:26
floatX	reinforcepy/handlers/experience_replay/dataset.py	/^floatX = 'float32'$/;"	kind:variable	line:9
font	examples/ALE/Human_Supervision/generate_human_dataset.py	/^    font = pygame.font.SysFont("Ubuntu Mono",25)$/;"	kind:variable	line:145
font	examples/ALE/Human_Supervision/generate_human_dataset.py	/^    font = pygame.font.SysFont("Ubuntu Mono",30)$/;"	kind:variable	line:165
font	examples/ALE/Human_Supervision/generate_human_dataset.py	/^    font = pygame.font.SysFont("Ubuntu Mono",32)$/;"	kind:variable	line:141
font	examples/ALE/Human_Supervision/generate_human_dataset.py	/^    font = pygame.font.SysFont("Ubuntu Mono",32)$/;"	kind:variable	line:158
frame_number	examples/ALE/Human_Supervision/generate_human_dataset.py	/^        frame_number = ale.getFrameNumber()$/;"	kind:variable	line:187
framebuffer	reinforcepy/test/test_framebuffer.py	/^def framebuffer():$/;"	kind:function	line:7
framebuffer.py	reinforcepy/handlers/framebuffer.py	1;"	kind:file	line:1
frames	examples/ALE/Human_Supervision/generate_human_dataset.py	/^    frames = list()$/;"	kind:variable	line:113
frames	examples/ALE/Human_Supervision/generate_human_dataset.py	/^    frames = np.swapaxes(np.asarray(frames),0,2)$/;"	kind:variable	line:127
frames	examples/ALE/Human_Supervision/generate_human_dataset.py	/^    frames = surfarray.make_surface(frames)$/;"	kind:variable	line:131
fromDict	reinforcepy/handlers/parameters.py	/^    def fromDict(in_dict, extra_fns={}):$/;"	kind:member	line:92
fromJSON	reinforcepy/handlers/parameters.py	/^    def fromJSON(filename, extra_fns={}):$/;"	kind:member	line:85
game_surface	examples/ALE/Human_Supervision/generate_human_dataset.py	/^game_surface = pygame.Surface((screen_width,screen_height))$/;"	kind:variable	line:74
gamescreen	examples/ALE/Human_Supervision/generate_human_dataset.py	/^        gamescreen = ale.getScreenGrayscale(gamescreen)$/;"	kind:variable	line:117
gamescreen	examples/ALE/Human_Supervision/generate_human_dataset.py	/^    gamescreen = None$/;"	kind:variable	line:115
generate_human_dataset.py	examples/ALE/Human_Supervision/generate_human_dataset.py	1;"	kind:file	line:1
get	reinforcepy/handlers/parameters.py	/^    def get(self, key):$/;"	kind:member	line:35
get_accumulated_gradients	reinforcepy/networks/dqn/theanolasagne/async_target_cnn.py	/^    def get_accumulated_gradients(self):$/;"	kind:member	line:136
get_action	reinforcepy/handlers/actionhandler.py	/^    def get_action(self, action_values, random=True):$/;"	kind:member	line:61
get_action	reinforcepy/learners/TableLookup/qtable.py	/^    def get_action(self, state):$/;"	kind:member	line:17
get_action	reinforcepy/learners/base_q_learner.py	/^    def get_action(self, state):$/;"	kind:member	line:8
get_action	reinforcepy/learners/base_sarsa_learner.py	/^    def get_action(self, state):$/;"	kind:member	line:8
get_action	reinforcepy/learners/dqn/asynchronous/base_thread_learner.py	/^    def get_action(self, state):$/;"	kind:member	line:69
get_action	reinforcepy/learners/dqn/dqn.py	/^    def get_action(self, state):$/;"	kind:member	line:41
get_action_from_probabilities	reinforcepy/networks/dqn/tflow/nstep_a3c.py	/^def get_action_from_probabilities(cnn_action_probabilities):$/;"	kind:function	line:153
get_action_from_probabilities	reinforcepy/networks/dqn/tflow/nstep_a3c_lstm.py	/^def get_action_from_probabilities(cnn_action_probabilities):$/;"	kind:function	line:201
get_action_values	reinforcepy/learners/dqn/dqn.py	/^    def get_action_values(self, processed_screens):$/;"	kind:member	line:74
get_buffer	reinforcepy/handlers/framebuffer.py	/^    def get_buffer(self):$/;"	kind:member	line:60
get_buffer_with	reinforcepy/handlers/framebuffer.py	/^    def get_buffer_with(self, state):$/;"	kind:member	line:38
get_gradients	reinforcepy/networks/dqn/theanolasagne/async_a3c_cnn.py	/^    def get_gradients(self):$/;"	kind:member	line:144
get_gradients	reinforcepy/networks/dqn/theanolasagne/async_target_cnn.py	/^    def get_gradients(self, state, action, reward, state_tp1, terminal):$/;"	kind:member	line:98
get_grads_fn	reinforcepy/networks/dqn/theanolasagne/async_target_cnn.py	/^    def get_grads_fn(self, loss_shared_vars, grads):$/;"	kind:member	line:160
get_grads_fn	reinforcepy/networks/dqn/theanolasagne/async_target_cnn.py	/^    def get_grads_fn(self, loss_shared_vars, grads):$/;"	kind:member	line:201
get_grads_fn	reinforcepy/networks/dqn/theanolasagne/async_target_cnn.py	/^    def get_grads_fn(self, loss_shared_vars, grads):$/;"	kind:member	line:83
get_input_space	reinforcepy/environments/openaigym/gym_atari.py	/^    def get_input_space(self):$/;"	kind:member	line:27
get_input_space	reinforcepy/environments/openaigym/gym_wrapper.py	/^    def get_input_space(self):$/;"	kind:member	line:26
get_lasagne_conv_layer	reinforcepy/networks/dqn/theanolasagne/dqn_inits.py	/^def get_lasagne_conv_layer():$/;"	kind:function	line:92
get_legal_actions	reinforcepy/environments/base_environment.py	/^    def get_legal_actions(self):$/;"	kind:member	line:48
get_legal_actions	reinforcepy/environments/openaigym/gym_atari.py	/^    def get_legal_actions(self):$/;"	kind:member	line:30
get_legal_actions	reinforcepy/environments/openaigym/gym_wrapper.py	/^    def get_legal_actions(self):$/;"	kind:member	line:29
get_lstm_state	reinforcepy/networks/dqn/tflow/nstep_a3c_lstm.py	/^    def get_lstm_state(self):$/;"	kind:member	line:197
get_minibatch_vars	reinforcepy/learners/dqn/asynchronous/onestep_sarsa_thread_learner.py	/^    def get_minibatch_vars(self):$/;"	kind:member	line:63
get_minibatch_vars	reinforcepy/learners/dqn/asynchronous/q_thread_learner.py	/^    def get_minibatch_vars(self):$/;"	kind:member	line:54
get_num_actions	reinforcepy/environments/ALE/ALE_environment.py	/^    def get_num_actions(self):$/;"	kind:member	line:92
get_output	reinforcepy/networks/dqn/base_network.py	/^    def get_output(self, x):$/;"	kind:member	line:40
get_output	reinforcepy/networks/dqn/tflow/dqn_inits.py	/^    def get_output(sess, state):$/;"	kind:function	line:81
get_output	reinforcepy/networks/dqn/tflow/dqn_nips.py	/^    def get_output(self, state):$/;"	kind:member	line:56
get_output	reinforcepy/networks/dqn/tflow/nstep_a3c.py	/^        def get_output(sess, state):$/;"	kind:function	line:114
get_output	reinforcepy/networks/dqn/tflow/nstep_a3c_lstm.py	/^        def get_output(sess, state):$/;"	kind:function	line:145
get_output	reinforcepy/networks/dqn/tflow/one_step_target_sarsa.py	/^        def get_output(sess, state):$/;"	kind:function	line:152
get_output	reinforcepy/networks/dqn/tflow/one_step_target_sarsa.py	/^    def get_output(self, x):$/;"	kind:member	line:175
get_output	reinforcepy/networks/dqn/theanolasagne/async_target_cnn.py	/^    def get_output(self, state):$/;"	kind:member	line:105
get_output	reinforcepy/networks/dqn/theanolasagne/dqn_nips.py	/^    def get_output(self, state):$/;"	kind:member	line:64
get_parameters	reinforcepy/networks/dqn/theanolasagne/async_a3c_cnn.py	/^    def get_parameters(self):$/;"	kind:member	line:140
get_parameters	reinforcepy/networks/dqn/theanolasagne/async_target_cnn.py	/^    def get_parameters(self):$/;"	kind:member	line:127
get_policy_output	reinforcepy/networks/dqn/theanolasagne/async_a3c_cnn.py	/^    def get_policy_output(self, state):$/;"	kind:member	line:106
get_random	reinforcepy/handlers/actionhandler.py	/^    def get_random(self):$/;"	kind:member	line:118
get_random_action	reinforcepy/handlers/actionhandler.py	/^    def get_random_action(self):$/;"	kind:member	line:139
get_size	reinforcepy/handlers/experience_replay/prioritized/binarytree.py	/^    def get_size(self):$/;"	kind:member	line:117
get_size	reinforcepy/handlers/experience_replay/prioritized/binarytree.py	/^    def get_size(self):$/;"	kind:member	line:63
get_state	reinforcepy/environments/ALE/ALE_environment.py	/^    def get_state(self):$/;"	kind:member	line:77
get_state	reinforcepy/environments/base_environment.py	/^    def get_state(self):$/;"	kind:member	line:42
get_state	reinforcepy/environments/openaigym/gym_atari.py	/^    def get_state(self):$/;"	kind:member	line:11
get_state	reinforcepy/environments/openaigym/gym_wrapper.py	/^    def get_state(self):$/;"	kind:member	line:10
get_state_shape	reinforcepy/environments/ALE/ALE_environment.py	/^    def get_state_shape(self):$/;"	kind:member	line:83
get_status	reinforcepy/learners/dqn/dqn.py	/^    def get_status(self):$/;"	kind:member	line:77
get_target_output	reinforcepy/networks/dqn/theanolasagne/async_target_cnn.py	/^    def get_target_output(self, state):$/;"	kind:member	line:109
get_terminal	reinforcepy/environments/ALE/ALE_environment.py	/^    def get_terminal(self):$/;"	kind:member	line:86
get_terminal	reinforcepy/environments/base_environment.py	/^    def get_terminal(self):$/;"	kind:member	line:31
get_terminal	reinforcepy/environments/openaigym/gym_atari.py	/^    def get_terminal(self):$/;"	kind:member	line:18
get_terminal	reinforcepy/environments/openaigym/gym_wrapper.py	/^    def get_terminal(self):$/;"	kind:member	line:17
get_value_output	reinforcepy/networks/dqn/theanolasagne/async_a3c_cnn.py	/^    def get_value_output(self, state):$/;"	kind:member	line:110
get_yx_vals	reinforcepy/handlers/experience_replay/prioritized/binarytree.py	/^    def get_yx_vals(self, node_yx_vals, loc, depth, curr_depth=0):$/;"	kind:member	line:71
gradient_step	reinforcepy/networks/dqn/theanolasagne/async_a3c_cnn.py	/^    def gradient_step(self, gradients):$/;"	kind:member	line:114
gradient_step	reinforcepy/networks/dqn/theanolasagne/async_target_cnn.py	/^    def gradient_step(self, gradients, learning_rate: float):$/;"	kind:member	line:113
gym_atari.py	reinforcepy/environments/openaigym/gym_atari.py	1;"	kind:file	line:1
gym_wrapper.py	reinforcepy/environments/openaigym/gym_wrapper.py	1;"	kind:file	line:1
has	reinforcepy/handlers/parameters.py	/^    def has(self, key):$/;"	kind:member	line:68
height	examples/ALE/Human_Supervision/generate_human_dataset.py	/^    height = font.get_height()*1.2$/;"	kind:variable	line:146
height	examples/ALE/Human_Supervision/generate_human_dataset.py	/^    height = font.get_height()*1.2$/;"	kind:variable	line:160
here	setup.py	/^here = os.path.abspath(os.path.dirname(__file__))$/;"	kind:variable	line:9
html_static_path	docs/conf.py	/^html_static_path = ['_static']$/;"	kind:variable	line:156
html_theme	docs/conf.py	/^html_theme = 'sphinx_rtd_theme'$/;"	kind:variable	line:127
html_theme_path	docs/conf.py	/^html_theme_path = [sphinx_rtd_theme.get_html_theme_path()]$/;"	kind:variable	line:135
htmlhelp_basename	docs/conf.py	/^htmlhelp_basename = 'reinforcePydoc'$/;"	kind:variable	line:219
include_package_data	setup.py	/^    include_package_data=True,$/;"	kind:variable	line:49
init_tf_session	reinforcepy/networks/dqn/base_network.py	/^    def init_tf_session(self):$/;"	kind:member	line:28
init_tf_session	reinforcepy/networks/dqn/tflow/one_step_target_sarsa.py	/^    def init_tf_session(self):$/;"	kind:member	line:37
insert	reinforcepy/handlers/experience_replay/prioritized/binarytree.py	/^    def insert(self, new_val, extra_vals=None):$/;"	kind:member	line:12
insert	reinforcepy/handlers/experience_replay/prioritized/binarytree.py	/^    def insert(self, val, extra_vals=None):$/;"	kind:member	line:96
key_action_tform_table	examples/ALE/Human_Supervision/generate_human_dataset.py	/^key_action_tform_table = ($/;"	kind:variable	line:21
keys	examples/ALE/Human_Supervision/generate_human_dataset.py	/^    keys = 0$/;"	kind:variable	line:96
language	docs/conf.py	/^language = None$/;"	kind:variable	line:83
last_phi	reinforcepy/handlers/experience_replay/dataset.py	/^    def last_phi(self):$/;"	kind:member	line:75
latex_documents	docs/conf.py	/^latex_documents = [$/;"	kind:variable	line:240
latex_elements	docs/conf.py	/^latex_elements = {$/;"	kind:variable	line:223
learner	_experiments/test_api.py	/^learner = Single\/Multithread Dqn,Sarsa (policySelectMax=True, Recurrent=True)$/;"	kind:variable	line:5
legal_actions	examples/ALE/Human_Supervision/generate_human_dataset.py	/^legal_actions = ale.getMinimalActionSet()$/;"	kind:variable	line:61
license	setup.py	/^    license='GNU License',$/;"	kind:variable	line:40
line_pos	examples/ALE/Human_Supervision/generate_human_dataset.py	/^    line_pos = 40$/;"	kind:variable	line:148
linear_annealer.py	reinforcepy/handlers/linear_annealer.py	1;"	kind:file	line:1
load	reinforcepy/networks/dqn/base_network.py	/^    def load(self, path):$/;"	kind:member	line:49
load	reinforcepy/networks/dqn/tflow/dqn_nips.py	/^    def load(self, filename):$/;"	kind:member	line:59
load	reinforcepy/networks/dqn/tflow/one_step_target_sarsa.py	/^    def load(self, path):$/;"	kind:member	line:188
load	reinforcepy/networks/dqn/theanolasagne/dqn_nips.py	/^    def load(self, filename):$/;"	kind:member	line:68
load_config	examples/ALE/DQN/run_dqn_experiment.py	/^def load_config():$/;"	kind:function	line:8
load_config	examples/ALE/DQN/run_dqn_viewer.py	/^def load_config():$/;"	kind:function	line:7
load_config	examples/ALE/DQN/runs/dqn_spragunr_breakout_terminallossoflife/run_dqn_experiment.py	/^def load_config():$/;"	kind:function	line:7
long_description	setup.py	/^    long_description=long_description,$/;"	kind:variable	line:47
long_description	setup.py	/^long_description = read('README.md')$/;"	kind:variable	line:22
loss	examples/ALE/transfer_learning/transfertest.py	/^            loss = tf.reduce_mean(tf.square(diff))$/;"	kind:variable	line:19
loss_fn	reinforcepy/networks/dqn/theanolasagne/async_target_cnn.py	/^    def loss_fn(self, net_output, target_output, loss_shared_vars, discount):$/;"	kind:member	line:151
loss_fn	reinforcepy/networks/dqn/theanolasagne/async_target_cnn.py	/^    def loss_fn(self, net_output, target_output, loss_shared_vars, discount):$/;"	kind:member	line:198
loss_fn	reinforcepy/networks/dqn/theanolasagne/async_target_cnn.py	/^    def loss_fn(self, net_output, target_output, loss_shared_vars, discount):$/;"	kind:member	line:75
main	examples/ALE/DQN/run_dqn_experiment.py	/^def main(experiment_parameters):$/;"	kind:function	line:16
main	examples/ALE/DQN/run_dqn_viewer.py	/^def main(model_path, experiment_parameters):$/;"	kind:function	line:15
main	examples/ALE/DQN/runs/dqn_spragunr_breakout_terminallossoflife/run_dqn_experiment.py	/^def main(experiment_parameters):$/;"	kind:function	line:22
main	examples/ALE/DQN_Async/run_a3c.py	/^def main(rom_args, learner_args, network_args, num_threads, epochs, logdir, save_interval):$/;"	kind:function	line:9
main	examples/ALE/DQN_Async/run_a3c_lstm.py	/^def main(rom_args, learner_args, network_args, num_threads, epochs, logdir, save_interval):$/;"	kind:function	line:9
main	examples/ALE/DQN_Async/run_a3c_viewer.py	/^def main(model_path, rom_args, learner_args, network_args, num_threads, initial_learning_rate, epochs, logdir, save_interval, deterministic=True):$/;"	kind:function	line:12
main	examples/ALE/DQN_Async/run_dqn.py	/^def main(rom_args, learner_args, network_args, algorithm_type, num_threads, epochs, logdir, save_interval):$/;"	kind:function	line:10
main	examples/ALE/DQN_Async/run_dqn_viewer.py	/^def main(model_path, rom_args, learner_args, network_args, num_threads, epochs, logdir, save_interval):$/;"	kind:function	line:11
main	examples/ALE/DQN_Async/run_dqnexpreplay.py	/^def main(rom_args, learner_args, network_args, algorithm_type, num_threads, epochs, logdir, save_interval):$/;"	kind:function	line:10
main	examples/ALE/DQN_Async/run_onestep_sarsa.py	/^def main(rom_args, learner_args, network_args, num_threads, initial_learning_rate, epochs, logdir, save_interval):$/;"	kind:function	line:10
main	reinforcepy/handlers/experience_replay/dataset.py	/^def main():$/;"	kind:function	line:303
man_pages	docs/conf.py	/^man_pages = [$/;"	kind:variable	line:270
master_doc	docs/conf.py	/^master_doc = 'index'$/;"	kind:variable	line:62
max_size_tests	reinforcepy/handlers/experience_replay/dataset.py	/^def max_size_tests():$/;"	kind:function	line:253
minibatch_accumulate	reinforcepy/learners/dqn/asynchronous/onestep_sarsa_thread_learner.py	/^    def minibatch_accumulate(self, state, action, reward, state_tp1, action_tp1, terminal):$/;"	kind:member	line:47
minibatch_accumulate	reinforcepy/learners/dqn/asynchronous/q_thread_learner.py	/^    def minibatch_accumulate(self, state, action, reward, state_tp1, terminal):$/;"	kind:member	line:40
multithreadServer	_experiments/test_api.py	/^	multithreadServer = Multithread(learners)$/;"	kind:variable	line:8
n_out	examples/ALE/transfer_learning/transfertest.py	/^n_out = environment.get_num_actions()$/;"	kind:variable	line:8
name	setup.py	/^    name='ReinforcePy',$/;"	kind:variable	line:37
net	examples/ALE/transfer_learning/transfertest.py	/^            net = create_nips_network(net_a._t_x_input, n_out)$/;"	kind:variable	line:17
net_a	examples/ALE/transfer_learning/transfertest.py	/^net_a = TargetDQN([None, 4, 84, 84], n_out, 'nstep')$/;"	kind:variable	line:10
network	_experiments/test_api.py	/^network = OneStep\/Nstep Q\/A3C OneStepSarsa(target=True, Recurrent=True)$/;"	kind:variable	line:4
nn_layer	reinforcepy/networks/util/tflow_util.py	/^def nn_layer(input_tensor, shape, layer_name, act, conv, stride=None, variable_summaries=True):$/;"	kind:function	line:22
node	reinforcepy/test/test_binarytree.py	/^def node():$/;"	kind:function	line:7
nstep_a3c.py	reinforcepy/networks/dqn/tflow/nstep_a3c.py	1;"	kind:file	line:1
nstep_a3c_lstm.py	reinforcepy/networks/dqn/tflow/nstep_a3c_lstm.py	1;"	kind:file	line:1
one_hot	reinforcepy/networks/util/tflow_util.py	/^def one_hot(select_from_tensor, index_tensor, output_num):$/;"	kind:function	line:109
one_step_target_sarsa.py	reinforcepy/networks/dqn/tflow/one_step_target_sarsa.py	1;"	kind:file	line:1
onestep_sarsa_thread_learner.py	reinforcepy/learners/dqn/asynchronous/onestep_sarsa_thread_learner.py	1;"	kind:file	line:1
output_reward	reinforcepy/learners/dqn/asynchronous/async_thread_host.py	/^        def output_reward(r):$/;"	kind:function	line:22
packages	setup.py	/^    packages=['reinforcepy'],$/;"	kind:variable	line:48
parameters.py	reinforcepy/handlers/parameters.py	1;"	kind:file	line:1
phi	reinforcepy/handlers/experience_replay/dataset.py	/^    def phi(self, img):$/;"	kind:member	line:80
platforms	setup.py	/^    platforms='any',$/;"	kind:variable	line:50
plot	reinforcepy/handlers/experience_replay/prioritized/binarytree.py	/^    def plot(self):$/;"	kind:member	line:120
pop_max	reinforcepy/handlers/experience_replay/prioritized/binarytree.py	/^    def pop_max(self):$/;"	kind:member	line:103
pop_max	reinforcepy/handlers/experience_replay/prioritized/binarytree.py	/^    def pop_max(self):$/;"	kind:member	line:24
pop_min	reinforcepy/handlers/experience_replay/prioritized/binarytree.py	/^    def pop_min(self):$/;"	kind:member	line:110
pop_min	reinforcepy/handlers/experience_replay/prioritized/binarytree.py	/^    def pop_min(self):$/;"	kind:member	line:39
possible_update_target_network	reinforcepy/networks/dqn/tflow/target_dqn.py	/^    def possible_update_target_network(self, global_step):$/;"	kind:member	line:208
pressed	examples/ALE/Human_Supervision/generate_human_dataset.py	/^    pressed = pygame.key.get_pressed()$/;"	kind:variable	line:97
print_status	reinforcepy/learners/Novelty/NoveltyHost.py	/^    def print_status(self, st):$/;"	kind:member	line:10
print_table	examples/SimpleQLearn/FrozenLakev0.py	/^def print_table(learner):$/;"	kind:function	line:41
processedImg	examples/ALE/Human_Supervision/generate_human_dataset.py	/^        processedImg = gamescreen[33:-16, :, 0]$/;"	kind:variable	line:119
project	docs/conf.py	/^project = 'reinforcePy'$/;"	kind:variable	line:65
pygments_style	docs/conf.py	/^pygments_style = 'sphinx'$/;"	kind:variable	line:111
q_thread_learner.py	reinforcepy/learners/dqn/asynchronous/q_thread_learner.py	1;"	kind:file	line:1
qtable	examples/SimpleQLearn/FrozenLakev0.py	/^qtable = QTable(env.get_legal_actions(), anneal_tuple)$/;"	kind:variable	line:11
qtable.py	reinforcepy/learners/TableLookup/qtable.py	1;"	kind:file	line:1
ram	examples/ALE/Human_Supervision/generate_human_dataset.py	/^    ram = np.zeros((ram_size),dtype=np.uint8)$/;"	kind:variable	line:136
ram_pos	examples/ALE/Human_Supervision/generate_human_dataset.py	/^    ram_pos = 0$/;"	kind:variable	line:149
ram_size	examples/ALE/Human_Supervision/generate_human_dataset.py	/^    ram_size = ale.getRAMSize()$/;"	kind:variable	line:135
ram_string	examples/ALE/Human_Supervision/generate_human_dataset.py	/^        ram_string = ''.join(["%02X "%ram[x] for x in range(ram_pos,min(ram_pos+16,128))])$/;"	kind:variable	line:151
randVals	reinforcepy/handlers/actionhandler.py	/^    randVals = 2$/;"	kind:variable	line:17
random_batch	reinforcepy/handlers/experience_replay/dataset.py	/^    def random_batch(self, batch_size):$/;"	kind:member	line:94
random_sequential_batch	reinforcepy/handlers/experience_replay/dataset.py	/^    def random_sequential_batch(self, batch_size):$/;"	kind:member	line:140
read	setup.py	/^def read(*filenames, **kwargs):$/;"	kind:function	line:12
recurrent_thread_learner.py	reinforcepy/learners/dqn/asynchronous/recurrent_thread_learner.py	1;"	kind:file	line:1
release	docs/conf.py	/^release = '0.1'$/;"	kind:variable	line:76
render	reinforcepy/environments/openaigym/gym_atari.py	/^    def render(self):$/;"	kind:member	line:33
render	reinforcepy/environments/openaigym/gym_wrapper.py	/^    def render(self):$/;"	kind:member	line:32
required	reinforcepy/handlers/parameters.py	/^    def required(self, req_parm_list):$/;"	kind:member	line:18
reset	reinforcepy/environments/ALE/ALE_environment.py	/^    def reset(self):$/;"	kind:member	line:54
reset	reinforcepy/environments/base_environment.py	/^    def reset(self):$/;"	kind:member	line:25
reset	reinforcepy/environments/openaigym/gym_atari.py	/^    def reset(self):$/;"	kind:member	line:14
reset	reinforcepy/environments/openaigym/gym_wrapper.py	/^    def reset(self):$/;"	kind:member	line:13
reset	reinforcepy/handlers/framebuffer.py	/^    def reset(self):$/;"	kind:member	line:54
reset	reinforcepy/learners/base_q_learner.py	/^    def reset(self):$/;"	kind:member	line:5
reset	reinforcepy/learners/base_sarsa_learner.py	/^    def reset(self):$/;"	kind:member	line:5
reset	reinforcepy/learners/dqn/asynchronous/base_thread_learner.py	/^    def reset(self):$/;"	kind:member	line:41
reset	reinforcepy/learners/dqn/asynchronous/recurrent_thread_learner.py	/^    def reset(self):$/;"	kind:member	line:10
reset	reinforcepy/networks/dqn/base_network.py	/^    def reset(self):$/;"	kind:member	line:52
reset_lstm_state	reinforcepy/networks/dqn/tflow/nstep_a3c_lstm.py	/^        def reset_lstm_state(new_state=None):$/;"	kind:function	line:182
reset_minibatch	reinforcepy/learners/dqn/asynchronous/base_thread_learner.py	/^    def reset_minibatch(self):$/;"	kind:member	line:83
reset_minibatch	reinforcepy/learners/dqn/asynchronous/onestep_sarsa_thread_learner.py	/^    def reset_minibatch(self):$/;"	kind:member	line:55
reset_minibatch	reinforcepy/learners/dqn/asynchronous/q_thread_learner.py	/^    def reset_minibatch(self):$/;"	kind:member	line:47
reward	examples/ALE/Human_Supervision/generate_human_dataset.py	/^    reward = 0$/;"	kind:variable	line:114
rewards	examples/ALE/Human_Supervision/generate_human_dataset.py	/^rewards = list()$/;"	kind:variable	line:86
rom	examples/ALE/Human_Supervision/generate_human_dataset.py	/^rom = b'D:\\\\_code\\\\montezuma_revenge.bin'$/;"	kind:variable	line:58
run	reinforcepy/learners/dqn/asynchronous/base_thread_learner.py	/^    def run(self):$/;"	kind:member	line:51
run_a3c.py	examples/ALE/DQN_Async/run_a3c.py	1;"	kind:file	line:1
run_a3c_lstm.py	examples/ALE/DQN_Async/run_a3c_lstm.py	1;"	kind:file	line:1
run_a3c_viewer.py	examples/ALE/DQN_Async/run_a3c_viewer.py	1;"	kind:file	line:1
run_date	examples/ALE/DQN_Async/run_a3c.py	/^    run_date = datetime.datetime.now().strftime("%m-%d-%Y-%H-%M")$/;"	kind:variable	line:35
run_date	examples/ALE/DQN_Async/run_a3c_lstm.py	/^    run_date = datetime.datetime.now().strftime("%m-%d-%Y-%H-%M")$/;"	kind:variable	line:35
run_date	examples/ALE/DQN_Async/run_dqn.py	/^    run_date = datetime.datetime.now().strftime("%m-%d-%Y-%H-%M")$/;"	kind:variable	line:42
run_date	examples/ALE/DQN_Async/run_dqnexpreplay.py	/^    run_date = datetime.datetime.now().strftime("%m-%d-%Y-%H-%M")$/;"	kind:variable	line:42
run_dqn.py	examples/ALE/DQN_Async/run_dqn.py	1;"	kind:file	line:1
run_dqn_experiment.py	examples/ALE/DQN/run_dqn_experiment.py	1;"	kind:file	line:1
run_dqn_experiment.py	examples/ALE/DQN/runs/dqn_spragunr_breakout_terminallossoflife/run_dqn_experiment.py	1;"	kind:file	line:1
run_dqn_viewer.py	examples/ALE/DQN/run_dqn_viewer.py	1;"	kind:file	line:1
run_dqn_viewer.py	examples/ALE/DQN_Async/run_dqn_viewer.py	1;"	kind:file	line:1
run_dqnexpreplay.py	examples/ALE/DQN_Async/run_dqnexpreplay.py	1;"	kind:file	line:1
run_episode	reinforcepy/learners/TableLookup/sarsatable.py	/^    def run_episode(self, environment):$/;"	kind:member	line:9
run_episode	reinforcepy/learners/base_learner.py	/^    def run_episode(self, environment):$/;"	kind:member	line:8
run_episode	reinforcepy/learners/base_q_learner.py	/^    def run_episode(self, environment):$/;"	kind:member	line:20
run_episode	reinforcepy/learners/base_sarsa_learner.py	/^    def run_episode(self, environment):$/;"	kind:member	line:20
run_epoch	reinforcepy/learners/dqn/dqn.py	/^    def run_epoch(self, environment, epoch_step_count=50000):$/;"	kind:member	line:34
run_epochs	reinforcepy/learners/dqn/asynchronous/async_thread_host.py	/^    def run_epochs(self, num_epochs, threaded_learners, EPOCH_DEF=1000000, save_interval=1):$/;"	kind:member	line:31
run_minibatch	reinforcepy/learners/dqn/dqn.py	/^    def run_minibatch(self):$/;"	kind:member	line:68
run_onestep_sarsa.py	examples/ALE/DQN_Async/run_onestep_sarsa.py	1;"	kind:file	line:1
run_tests	setup.py	/^    def run_tests(self):$/;"	kind:member	line:31
run_till_convergence	examples/SimpleQLearn/FrozenLakev0.py	/^def run_till_convergence(learner, env):$/;"	kind:function	line:17
run_type	examples/ALE/DQN_Async/run_dqn.py	/^        run_type = sys.argv[1]$/;"	kind:variable	line:39
run_type	examples/ALE/DQN_Async/run_dqn.py	/^    run_type = 'dqn'$/;"	kind:variable	line:35
run_type	examples/ALE/DQN_Async/run_dqnexpreplay.py	/^        run_type = sys.argv[1]$/;"	kind:variable	line:39
run_type	examples/ALE/DQN_Async/run_dqnexpreplay.py	/^    run_type = 'dqn'$/;"	kind:variable	line:35
sample	reinforcepy/networks/dqn/theanolasagne/dqn_inits.py	/^    def sample(self, shape):$/;"	kind:member	line:115
sarsatable	examples/SimpleQLearn/FrozenLakev0.py	/^sarsatable = SARSATable(env.get_legal_actions(), anneal_tuple)$/;"	kind:variable	line:12
sarsatable.py	reinforcepy/learners/TableLookup/sarsatable.py	1;"	kind:file	line:1
save	reinforcepy/learners/dqn/dqn.py	/^    def save(self, filename):$/;"	kind:member	line:83
save	reinforcepy/networks/dqn/base_network.py	/^    def save(self, *args, **kwargs):$/;"	kind:member	line:46
save	reinforcepy/networks/dqn/tflow/dqn_nips.py	/^    def save(self, filename):$/;"	kind:member	line:62
save	reinforcepy/networks/dqn/tflow/one_step_target_sarsa.py	/^    def save(self, *args, **kwargs):$/;"	kind:member	line:185
save	reinforcepy/networks/dqn/theanolasagne/dqn_nips.py	/^    def save(self, filename):$/;"	kind:member	line:75
screen	examples/ALE/Human_Supervision/generate_human_dataset.py	/^screen = pygame.display.set_mode((display_width,display_height))$/;"	kind:variable	line:71
set	reinforcepy/handlers/parameters.py	/^    def set(self, key, value):$/;"	kind:member	line:55
set_action_num	reinforcepy/learners/dqn/dqn.py	/^    def set_action_num(self, num_actions):$/;"	kind:member	line:80
set_buffer	reinforcepy/handlers/framebuffer.py	/^    def set_buffer(self, frame_buffer):$/;"	kind:member	line:66
set_parameters	reinforcepy/networks/dqn/theanolasagne/async_a3c_cnn.py	/^    def set_parameters(self, new_parms):$/;"	kind:member	line:127
set_parameters	reinforcepy/networks/dqn/theanolasagne/async_target_cnn.py	/^    def set_parameters(self, new_parms):$/;"	kind:member	line:124
set_target_parameters	reinforcepy/networks/dqn/theanolasagne/async_target_cnn.py	/^    def set_target_parameters(self, new_parms):$/;"	kind:member	line:130
setup.py	setup.py	1;"	kind:file	line:1
simple_tests	reinforcepy/handlers/experience_replay/dataset.py	/^def simple_tests():$/;"	kind:function	line:184
skipFrame	examples/ALE/Human_Supervision/generate_human_dataset.py	/^skipFrame = 3$/;"	kind:variable	line:81
source_suffix	docs/conf.py	/^source_suffix = '.rst'$/;"	kind:variable	line:56
speed_tests	reinforcepy/handlers/experience_replay/dataset.py	/^def speed_tests(sequential=False):$/;"	kind:function	line:209
states	examples/ALE/Human_Supervision/generate_human_dataset.py	/^states = list()$/;"	kind:variable	line:87
step	reinforcepy/environments/ALE/ALE_environment.py	/^    def step(self, action):$/;"	kind:member	line:59
step	reinforcepy/environments/base_environment.py	/^    def step(self, action: int):$/;"	kind:member	line:8
step	reinforcepy/environments/openaigym/gym_atari.py	/^    def step(self, action):$/;"	kind:member	line:21
step	reinforcepy/environments/openaigym/gym_wrapper.py	/^    def step(self, action):$/;"	kind:member	line:20
step	reinforcepy/learners/base_q_learner.py	/^    def step(self, environment_step_fn, action):$/;"	kind:member	line:11
step	reinforcepy/learners/base_sarsa_learner.py	/^    def step(self, environment_step_fn, action):$/;"	kind:member	line:11
str2byte	examples/ALE/DQN/runs/dqn_spragunr_breakout_terminallossoflife/run_dqn_experiment.py	/^    def str2byte(string):$/;"	kind:function	line:12
target_dqn.py	reinforcepy/networks/dqn/tflow/target_dqn.py	1;"	kind:file	line:1
templates_path	docs/conf.py	/^templates_path = ['_templates']$/;"	kind:variable	line:51
test_action_vect_to_game_action	reinforcepy/test/test_actionhandler.py	/^def test_action_vect_to_game_action(action_handler: ActionHandler):$/;"	kind:function	line:35
test_actionhandler.py	reinforcepy/test/test_actionhandler.py	1;"	kind:file	line:1
test_add_state_to_buffer	reinforcepy/test/test_framebuffer.py	/^def test_add_state_to_buffer(framebuffer: FrameBuffer):$/;"	kind:function	line:21
test_anneal	reinforcepy/test/test_actionhandler.py	/^def test_anneal(action_handler: ActionHandler):$/;"	kind:function	line:41
test_anneal_to	reinforcepy/test/test_actionhandler.py	/^def test_anneal_to(action_handler: ActionHandler):$/;"	kind:function	line:49
test_api.py	_experiments/test_api.py	1;"	kind:file	line:1
test_binary_tree_pop_max	reinforcepy/test/test_binarytree.py	/^def test_binary_tree_pop_max(btree: BinaryTree):$/;"	kind:function	line:91
test_binary_tree_pop_min	reinforcepy/test/test_binarytree.py	/^def test_binary_tree_pop_min(btree: BinaryTree):$/;"	kind:function	line:100
test_binarytree.py	reinforcepy/test/test_binarytree.py	1;"	kind:file	line:1
test_depth	reinforcepy/test/test_binarytree.py	/^def test_depth(node: Node):$/;"	kind:function	line:26
test_framebuffer.py	reinforcepy/test/test_framebuffer.py	1;"	kind:file	line:1
test_game_action_to_action_ind	reinforcepy/test/test_actionhandler.py	/^def test_game_action_to_action_ind(action_handler: ActionHandler):$/;"	kind:function	line:29
test_get_action	reinforcepy/test/test_actionhandler.py	/^def test_get_action(action_handler: ActionHandler):$/;"	kind:function	line:21
test_get_buffer	reinforcepy/test/test_framebuffer.py	/^def test_get_buffer(framebuffer: FrameBuffer):$/;"	kind:function	line:38
test_get_buffer_with	reinforcepy/test/test_framebuffer.py	/^def test_get_buffer_with(framebuffer: FrameBuffer):$/;"	kind:function	line:12
test_get_random	reinforcepy/test/test_actionhandler.py	/^def test_get_random(action_handler: ActionHandler):$/;"	kind:function	line:68
test_insert	reinforcepy/test/test_binarytree.py	/^def test_insert(node: Node):$/;"	kind:function	line:12
test_memory_usage_ok	reinforcepy/handlers/experience_replay/dataset.py	/^def test_memory_usage_ok():$/;"	kind:function	line:274
test_pop_max	reinforcepy/test/test_binarytree.py	/^def test_pop_max(node: Node):$/;"	kind:function	line:41
test_pop_min	reinforcepy/test/test_binarytree.py	/^def test_pop_min(node: Node):$/;"	kind:function	line:55
test_rand_vals	reinforcepy/test/test_actionhandler.py	/^def test_rand_vals():$/;"	kind:function	line:84
test_random_sequential_batch	reinforcepy/handlers/experience_replay/dataset.py	/^def test_random_sequential_batch():$/;"	kind:function	line:295
test_set_buffer	reinforcepy/test/test_framebuffer.py	/^def test_set_buffer(framebuffer: FrameBuffer):$/;"	kind:function	line:42
test_set_legal_actions	reinforcepy/test/test_actionhandler.py	/^def test_set_legal_actions(action_handler: ActionHandler):$/;"	kind:function	line:12
test_size	reinforcepy/test/test_binarytree.py	/^def test_size(node: Node):$/;"	kind:function	line:30
test_suite	setup.py	/^    test_suite='reinforcepy.test.test_reinforcepy',$/;"	kind:variable	line:51
test_update_extra_vals	reinforcepy/test/test_binarytree.py	/^def test_update_extra_vals(node: Node):$/;"	kind:function	line:69
test_yx_vals	reinforcepy/test/test_binarytree.py	/^def test_yx_vals(node: Node):$/;"	kind:function	line:34
tests_require	setup.py	/^    tests_require=['pytest'],$/;"	kind:variable	line:42
texinfo_documents	docs/conf.py	/^texinfo_documents = [$/;"	kind:variable	line:284
text	examples/ALE/Human_Supervision/generate_human_dataset.py	/^        text = font.render(ram_string,1,(255,255,255))$/;"	kind:variable	line:152
text	examples/ALE/Human_Supervision/generate_human_dataset.py	/^    text = font.render("Current Action: " + str(a) ,1,(208,208,255))$/;"	kind:variable	line:159
text	examples/ALE/Human_Supervision/generate_human_dataset.py	/^    text = font.render("RAM: " ,1,(255,208,208))$/;"	kind:variable	line:142
text	examples/ALE/Human_Supervision/generate_human_dataset.py	/^    text = font.render("Total Reward: " + str(total_reward) ,1,(208,255,255))$/;"	kind:variable	line:166
tflow_util.py	reinforcepy/networks/util/tflow_util.py	1;"	kind:file	line:1
todo_include_todos	docs/conf.py	/^todo_include_todos = True$/;"	kind:variable	line:120
torch_init	reinforcepy/networks/util/tflow_util.py	/^def torch_init(input_tensor):$/;"	kind:function	line:103
total_reward	examples/ALE/Human_Supervision/generate_human_dataset.py	/^        total_reward = 0.0$/;"	kind:variable	line:191
total_reward	examples/ALE/Human_Supervision/generate_human_dataset.py	/^total_reward = 0.0$/;"	kind:variable	line:83
train	reinforcepy/networks/dqn/tflow/dqn_inits.py	/^    def train(sess, learning_rate, state, action, reward, state_tp1, terminal, run_summaries=False, **kwargs):$/;"	kind:function	line:68
train	reinforcepy/networks/dqn/tflow/dqn_nips.py	/^    def train(self, states, actions, rewards, state_tp1s, terminal):$/;"	kind:member	line:30
train	reinforcepy/networks/dqn/theanolasagne/dqn_nips.py	/^    def train(self, states, actions, rewards, state_tp1s, terminal):$/;"	kind:member	line:56
train_step	reinforcepy/networks/dqn/base_network.py	/^    def train_step(self, state, action, reward, state_tp1, terminal, global_step=None, summaries=False):$/;"	kind:member	line:43
train_step	reinforcepy/networks/dqn/tflow/nstep_a3c.py	/^        def train_step(sess, states, actions, rewards, states_tp1, terminals, global_step=0, summaries=False):$/;"	kind:function	line:120
train_step	reinforcepy/networks/dqn/tflow/nstep_a3c_lstm.py	/^        def train_step(sess, states, actions, rewards, states_tp1, terminals, lstm_state, global_step=0, summaries=False):$/;"	kind:function	line:152
train_step	reinforcepy/networks/dqn/tflow/nstep_a3c_lstm.py	/^    def train_step(self, state, action, reward, state_tp1, terminal, lstm_state=None, global_step=None, summaries=False):$/;"	kind:member	line:193
train_step	reinforcepy/networks/dqn/tflow/one_step_target_sarsa.py	/^        def train_step(sess, current_learning_rate, state, action, reward, state_tp1, action_tp1, terminal, summaries=False):$/;"	kind:function	line:157
train_step	reinforcepy/networks/dqn/tflow/one_step_target_sarsa.py	/^    def train_step(self, current_learning_rate, state, action, reward, state_tp1, action_tp1, terminal, summaries=False):$/;"	kind:member	line:179
transfertest.py	examples/ALE/transfer_learning/transfertest.py	1;"	kind:file	line:1
trivial_tests	reinforcepy/handlers/experience_replay/dataset.py	/^def trivial_tests():$/;"	kind:function	line:236
update	reinforcepy/learners/Novelty/NoveltyClient.py	/^    def update(self, state, action, reward, state_tp1, terminal):$/;"	kind:member	line:12
update	reinforcepy/learners/TableLookup/qtable.py	/^    def update(self, state, action, reward, state_tp1, terminal):$/;"	kind:member	line:35
update	reinforcepy/learners/TableLookup/sarsatable.py	/^    def update(self, state, action, reward, state_tp1, action_tp1, terminal):$/;"	kind:member	line:12
update	reinforcepy/learners/base_q_learner.py	/^    def update(self, state, action, reward, state_tp1, terminal):$/;"	kind:member	line:14
update	reinforcepy/learners/base_sarsa_learner.py	/^    def update(self, state, action, reward, state_tp1, action_tp1, terminal):$/;"	kind:member	line:14
update	reinforcepy/learners/dqn/asynchronous/base_thread_learner.py	/^    def update(self, *args, **kwargs):$/;"	kind:member	line:60
update	reinforcepy/learners/dqn/asynchronous/exp_replay_q_thread_learner.py	/^    def update(self, state, action, reward, state_tp1, terminal):$/;"	kind:member	line:14
update	reinforcepy/learners/dqn/asynchronous/onestep_sarsa_thread_learner.py	/^    def update(self, state, action, reward, state_tp1, action_tp1, terminal):$/;"	kind:member	line:8
update	reinforcepy/learners/dqn/asynchronous/q_thread_learner.py	/^    def update(self, state, action, reward, state_tp1, terminal):$/;"	kind:member	line:7
update	reinforcepy/learners/dqn/asynchronous/recurrent_thread_learner.py	/^    def update(self, state, action, reward, state_tp1, terminal):$/;"	kind:member	line:15
update	reinforcepy/learners/dqn/dqn.py	/^    def update(self, state, action, reward, state_tp1, terminal):$/;"	kind:member	line:54
update	reinforcepy/test/test_binarytree.py	/^    def update(e_val):$/;"	kind:function	line:73
update_extra_vals	reinforcepy/handlers/experience_replay/prioritized/binarytree.py	/^    def update_extra_vals(self, compare_fn, update_fn):$/;"	kind:member	line:133
update_extra_vals	reinforcepy/handlers/experience_replay/prioritized/binarytree.py	/^    def update_extra_vals(self, compare_fn, update_fn):$/;"	kind:member	line:54
update_target_net	reinforcepy/networks/dqn/tflow/one_step_target_sarsa.py	/^        def update_target_net(sess):$/;"	kind:function	line:167
update_target_net	reinforcepy/networks/dqn/tflow/target_dqn.py	/^    def update_target_net(self, sess):$/;"	kind:member	line:200
update_target_network	reinforcepy/networks/dqn/tflow/one_step_target_sarsa.py	/^    def update_target_network(self):$/;"	kind:member	line:182
url	setup.py	/^    url='http:\/\/github.com\/islandman93\/reinforcepy',$/;"	kind:variable	line:39
validate_parms	reinforcepy/networks/dqn/theanolasagne/dqn_inits.py	/^def validate_parms(network_parms):$/;"	kind:function	line:105
version	docs/conf.py	/^version = '0.1'$/;"	kind:variable	line:74
version	setup.py	/^    version=0.2,$/;"	kind:variable	line:38
weight_variable_torch	reinforcepy/networks/util/tflow_util.py	/^def weight_variable_torch(name, shape, init='xavier', uniform=True):$/;"	kind:function	line:72
